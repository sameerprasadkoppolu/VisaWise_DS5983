{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Storage Context as Pinecone Index\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 100\n",
    "\n",
    "pc = Pinecone(api_key = \"bb081579-c077-4ca0-a107-c1a21d0e392c\") #Pinecone API\n",
    "\n",
    "pinecone_index = pc.Index(\"vector-only-rag-768\") #Pinecone Index\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Documents\n",
    "documents = SimpleDirectoryReader(input_dir = \"/Users/sameerprasadkoppolu/Desktop/MSDS Coursework/LLMs/Project/uscis_new\").load_data(num_workers=4, show_progress = True)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Embedding Model\n",
    "import torch \n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = \"Snowflake/snowflake-arctic-embed-m-v1.5\", trust_remote_code = True #768 Embedding Dimension\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create VectorStoreIndex - Run this only the first time when the vectors need to be stored in Pinecone DB\n",
    "# #Ensure that the vector dimension in the DB matches the d_model of your embedding model\n",
    "# #Default Chunk Size is 1024 size with default overlap of 200 - https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(documents, \n",
    "#                                         embed_model = Settings.embed_model,\n",
    "#                                         storage_context = storage_context, show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Vectors from Pinecone DB - Do this if the Pinecone Index already has vectors\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model = Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the LLM for Response Synthesis\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "Settings.llm = Ollama(model=\"llama3:instruct\", request_timeout=360.0, temperature = 0.3, num_beams = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Prompt Template, Retreiver, and Response Synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "import Stemmer\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    # \"Context information is below.\\n\"\n",
    "    # \"---------------------\\n\"\n",
    "    \"You are a USCIS policy helper\\n.\" \n",
    "    \"You will be provided with a query about USCIS policies and guidelines and you must answer it clearly and provide detailed steps using only the context information and not any prior knowledge\\n.\" \n",
    "    \"If the steps need to follow a certain order then ensure that the order is stated clearly. If any mathematical calculations need to be done make sure to show them clearly. If any forms need to be filed, make sure to specify what those forms are. Also cite any actual URLs if required to provide more clarity and make sure that these URLs are not broken.\\n\"\n",
    "    \"Remember to always answer the question as if you were chatting with a person.\"\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, answer the query\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k = 10,\n",
    "    embed_model = Settings.embed_model\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode = \"compact\", llm = Settings.llm, text_qa_template = qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Reranker\n",
    "#The Reranker here compares each of the top_k retreived vectors to the query vector, reranks them\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "rerank = FlagEmbeddingReranker(model=\"BAAI/bge-reranker-base\", top_n=5) #768 Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the Query Engine\n",
    "from IPython.display import display, Markdown\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors = [rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup LLM to allow for LLM only Response\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm_only = Ollama(model=\"llama3:instruct\", request_timeout=360.0, temperature = 0.3, num_beams = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Answers for Vector Rag and LLM on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Test Set\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"/Users/sameerprasadkoppolu/Desktop/MSDS Coursework/LLMs/Project/test_dataset/test_question_answers.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    message = df.loc[i, 'question']\n",
    "    df.loc[i, 'llm_only_response'] = llm_only.complete(message).text \n",
    "    df.loc[i, 'rag_response'] = query_engine.query(message).response \n",
    "\n",
    "df['embed_model'] = Settings.embed_model.model_name\n",
    "df['embedding_dimension'] = 768\n",
    "df['chunk_size'] = Settings.chunk_size \n",
    "df['chunk_overlap'] = Settings.chunk_overlap \n",
    "df['similarity_top_k'] = retriever.similarity_top_k \n",
    "df['rerank_top_n'] = rerank.top_n\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define file name for Results\n",
    "file_name = \"test_results_\" + \"llama_3_8B_instruct_\" + \"snowflake-arctic-embed-m-v1.5_\" +\"_768_1024_100_10_3.csv\"\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save File to Path\n",
    "df.to_csv(f\"/Users/sameerprasadkoppolu/Desktop/MSDS Coursework/LLMs/Project/test_dataset/Results/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Chat Engine for continued Conversations with Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Chat Engine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine.context import ContextChatEngine\n",
    "from llama_index.core.chat_engine.condense_plus_context import CondensePlusContextChatEngine\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=5000)\n",
    "\n",
    "chat_qa_prompt = PromptTemplate(\n",
    "    # \"Context information is below.\\n\"\n",
    "    # \"---------------------\\n\"\n",
    "    \"You are a USCIS policy helper. Always introduce yourself and be courteous and friendly.\\n.\" \n",
    "    \"You will be provided with a query about USCIS policies and guidelines and you must answer it clearly and provide detailed steps using only the context information and not any prior knowledge\\n.\" \n",
    "    \"If the steps need to follow a certain order then ensure that the order is stated clearly. If any mathematical calculations need to be done make sure to show them clearly. If any forms need to be filed, make sure to specify what those forms are. Also cite any actual URLs if required to provide more clarity and make sure that these URLs are not broken.\\n\"\n",
    "    \"Remember to always answer the question using the Context information to enhance your internal knowledge.\"\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    memory = memory,\n",
    "    retriever = retriever,\n",
    "    context_prompt = chat_qa_prompt,\n",
    "    node_postprocessors = [rerank],\n",
    "    response_synthesizer = response_synthesizer,\n",
    "    llm = Settings.llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to check memory (i.e. previous chat history)\n",
    "memory.chat_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chat with Chat Engine \n",
    "message = input()\n",
    "if memory.chat_store.store == {}:\n",
    "    chat_response = chat_engine.chat(message=message)\n",
    "else:\n",
    "    chat_response = chat_engine.chat(message=message,\n",
    "                                  chat_history = memory.chat_store.store['chat_history'],)\n",
    "display(Markdown(chat_response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reset (i.e. delete Chat Memory)\n",
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Answers for Context Based Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_df = pd.read_csv(\"/Users/sameerprasadkoppolu/Desktop/MSDS Coursework/LLMs/Project/test_dataset/responses_evaluation - responses_evaluation.csv\")\n",
    "context_df = context_df.iloc[:, :2]\n",
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(context_df))):\n",
    "    message = context_df.loc[i, 'Question']\n",
    "    context_df.loc[i, 'llm_only_response'] = llm_only.complete(message).text \n",
    "    context_df.loc[i, 'vector_rag_response'] = query_engine.query(message).response \n",
    "\n",
    "context_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating With RAGAS Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = input(\"Enter OPEN AI API Key: \")\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = Settings.llm\n",
    "\n",
    "critic_llm = Ollama(model=\"mistral-nemo\")\n",
    "embeddings = Settings.embed_model\n",
    "\n",
    "generator = TestsetGenerator.from_llama_index(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = generator.generate_with_llamaindex_docs(\n",
    "    documents,\n",
    "    test_size=5,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df.to_csv('Test set from RAGAS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    harmfulness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = testset.to_dataset()\n",
    "\n",
    "ds_dict = ds.to_dict()\n",
    "ds_dict[\"question\"]\n",
    "ds_dict[\"ground_truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = Ollama(model=\"llama3.1\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.llama_index import evaluate\n",
    "\n",
    "result_vector_only = evaluate(\n",
    "    query_engine=query_engine,\n",
    "    metrics=metrics,\n",
    "    dataset=ds_dict,\n",
    "    #llm=evaluator_llm,\n",
    "    embeddings=Settings.embed_model,\n",
    "    raise_exceptions=False,\n",
    "    #run_config=RunConfig(max_retries=3, max_wait=20)\n",
    ")\n",
    "\n",
    "print(result_vector_only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
